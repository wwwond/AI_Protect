{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 기반 네트워크 이상 탐지 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "\n",
    "# 불필요한 경고 메시지 무시 설정\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 파일 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FILE_PATH = \"../../data/traffic/CSE-CIC-IDS2018TrafficForML_CICFlowMeter_merged.csv\"\n",
    "SAMPLED_FILE_PATH = \"../../data/traffic/CSE-CIC-IDS2018_sampled_1_3.csv\"\n",
    "MODEL_PATH = \"../../model/traffic/traffic_model.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (최초 1회 실행) 원본 데이터 샘플링\n",
    "- **메모리 오류 방지**: 대용량 원본 파일을 작은 조각(청크)으로 나누어 읽고 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 파일이 없어 원본 데이터 샘플링을 시작합니다: ../../data/traffic/CSE-CIC-IDS2018TrafficForML_CICFlowMeter_merged.csv\n",
      "대용량 파일 청크 단위 샘플링 시작...\n",
      "  - 1번째 청크 처리 및 샘플링 중...\n",
      "  - 2번째 청크 처리 및 샘플링 중...\n",
      "  - 3번째 청크 처리 및 샘플링 중...\n",
      "  - 4번째 청크 처리 및 샘플링 중...\n",
      "  - 5번째 청크 처리 및 샘플링 중...\n",
      "  - 6번째 청크 처리 및 샘플링 중...\n",
      "  - 7번째 청크 처리 및 샘플링 중...\n",
      "  - 8번째 청크 처리 및 샘플링 중...\n",
      "  - 9번째 청크 처리 및 샘플링 중...\n",
      "  - 10번째 청크 처리 및 샘플링 중...\n",
      "  - 11번째 청크 처리 및 샘플링 중...\n",
      "  - 12번째 청크 처리 및 샘플링 중...\n",
      "  - 13번째 청크 처리 및 샘플링 중...\n",
      "  - 14번째 청크 처리 및 샘플링 중...\n",
      "  - 15번째 청크 처리 및 샘플링 중...\n",
      "  - 16번째 청크 처리 및 샘플링 중...\n",
      "  - 17번째 청크 처리 및 샘플링 중...\n",
      "\n",
      "샘플링 완료.\n",
      "샘플링 후 데이터 형태: (5411012, 80)\n",
      "샘플링된 데이터가 '../../data/traffic/CSE-CIC-IDS2018_sampled_1_3.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 샘플링된 파일이 이미 존재하면 이 단계는 건너뜁니다.\n",
    "if not os.path.exists(SAMPLED_FILE_PATH):\n",
    "    print(f\"샘플링 파일이 없어 원본 데이터 샘플링을 시작합니다: {RAW_FILE_PATH}\")\n",
    "\n",
    "    # 대용량 파일을 나눠 읽기 위한 청크 크기 설정\n",
    "    chunk_size = 1_000_000\n",
    "    sampled_chunks = []\n",
    "\n",
    "    try:\n",
    "        # chunksize 옵션을 사용하여 파일을 조각내어 읽는 객체 생성\n",
    "        reader = pd.read_csv(RAW_FILE_PATH, chunksize=chunk_size, low_memory=False)\n",
    "        \n",
    "        print(\"대용량 파일 청크 단위 샘플링 시작...\")\n",
    "        # 각 청크를 순회하며 처리\n",
    "        for i, chunk in enumerate(reader):\n",
    "            print(f\"  - {i+1}번째 청크 처리 및 샘플링 중...\")\n",
    "            \n",
    "            # 각 청크 내에서 1/3 비율로 계층적 샘플링 수행\n",
    "            _, chunk_sample = train_test_split(\n",
    "                chunk,\n",
    "                test_size=1/3,\n",
    "                random_state=0\n",
    "            )\n",
    "            sampled_chunks.append(chunk_sample)\n",
    "        \n",
    "        # 샘플링된 모든 청크들을 하나의 데이터프레임으로 병합\n",
    "        data_sample = pd.concat(sampled_chunks, ignore_index=True)\n",
    "        \n",
    "        print(\"\\n샘플링 완료.\")\n",
    "        print(f\"샘플링 후 데이터 형태: {data_sample.shape}\")\n",
    "        \n",
    "        # 최종 샘플링된 데이터 저장\n",
    "        os.makedirs(os.path.dirname(SAMPLED_FILE_PATH), exist_ok=True)\n",
    "        data_sample.to_csv(SAMPLED_FILE_PATH, index=False)\n",
    "        print(f\"샘플링된 데이터가 '{SAMPLED_FILE_PATH}'에 저장되었습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n파일 처리 중 오류가 발생했습니다: {e}\")\n",
    "        print(\"파일 경로가 정확한지, 파일에 손상이 없는지 확인해 주세요.\")\n",
    "\n",
    "else:\n",
    "    print(f\"'{os.path.basename(SAMPLED_FILE_PATH)}' 파일이 이미 존재하여 샘플링을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 로드, 전처리 및 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 데이터 처리 시작 ---\n",
      "데이터 로드 완료. 형태: (5411012, 22)\n",
      "컬럼명 변경 완료 (예: 'Dst Port' -> 'Dst_Port').\n",
      "훈련/검증 데이터 분할 완료. 훈련 데이터 형태: (4328794, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 데이터 처리 시작 ---\")\n",
    "\n",
    "\n",
    "# SELECTED_FEATURES_ORIGINAL = [\n",
    "#     # --- Packetbeat에서 직접 수집 가능한 필드 ---\n",
    "#     'Dst Port',\n",
    "#     'Protocol',\n",
    "#     'Flow Duration',\n",
    "#     'Tot Fwd Pkts',\n",
    "#     'Tot Bwd Pkts',\n",
    "#     'TotLen Fwd Pkts',\n",
    "#     'TotLen Bwd Pkts',\n",
    "    \n",
    "#     # --- 간단한 계산으로 생성 가능한 필드 ---\n",
    "#     'Flow Byts/s',\n",
    "#     'Flow Pkts/s',\n",
    "#     'Fwd Pkts/s',\n",
    "#     'Bwd Pkts/s',\n",
    "#     'Down/Up Ratio',\n",
    "#     'Pkt Size Avg',\n",
    "#     'Fwd Seg Size Avg', # 'Fwd Pkt Len Mean'의 근사치로 사용 가능\n",
    "#     'Bwd Seg Size Avg', # 'Bwd Pkt Len Mean'의 근사치로 사용 가능\n",
    "# ]\n",
    "SELECTED_FEATURES_ORIGINAL = [\n",
    "    'Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts', 'TotLen Bwd Pkts',\n",
    "    'Flow Byts/s', 'Flow Pkts/s', 'Bwd IAT Tot', 'FIN Flag Cnt', 'RST Flag Cnt',\n",
    "    'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'Down/Up Ratio',\n",
    "    'Pkt Size Avg', 'Fwd Seg Size Avg', 'Fwd Pkt Len Mean', 'Bwd Seg Size Avg', 'Bwd Pkt Len Mean'\n",
    "]\n",
    "\n",
    "# 사용할 컬럼('Label' 포함)만 지정하여 로드\n",
    "COLS_TO_LOAD = SELECTED_FEATURES_ORIGINAL + ['Label']\n",
    "data = pd.read_csv(SAMPLED_FILE_PATH, usecols=COLS_TO_LOAD, low_memory=False)\n",
    "print(f\"데이터 로드 완료. 형태: {data.shape}\")\n",
    "\n",
    "# 컬럼명의 공백을 언더스코어(_)로, 슬래시(/)를 _per_로 변경\n",
    "data.columns = data.columns.str.replace(' ', '_').str.replace('/', '_per_')\n",
    "print(\"컬럼명 변경 완료 (예: 'Dst Port' -> 'Dst_Port').\")\n",
    "\n",
    "# 변경된 컬럼명에 맞춰 피처 목록도 업데이트\n",
    "SELECTED_FEATURES_RENAMED = [col.replace(' ', '_').replace('/', '_per_') for col in SELECTED_FEATURES_ORIGINAL]\n",
    "\n",
    "# 레이블 통합\n",
    "label_mapping = {\n",
    "    'Benign': 'Benign', 'Bot': 'Bot', 'Infilteration': 'Bot',\n",
    "    'DoS attacks-SlowHTTPTest': 'DDoS', 'DoS attacks-Hulk': 'DDoS',\n",
    "    'DDoS attacks-LOIC-HTTP': 'DDoS', 'DoS attacks-GoldenEye': 'DDoS',\n",
    "    'DoS attacks-Slowloris': 'DDoS', 'DDOS attack-LOIC-UDP': 'DDoS',\n",
    "    'DDOS attack-HOIC': 'DDoS', 'Brute Force -Web': 'Brute Force',\n",
    "    'Brute Force -XSS': 'Brute Force', 'FTP-BruteForce': 'Brute Force',\n",
    "    'SSH-Bruteforce': 'Brute Force', 'SQL Injection': 'SQL Injection', 'XSS': 'XSS'\n",
    "}\n",
    "data['Label'] = data['Label'].map(label_mapping)\n",
    "filtered_data = data.dropna(subset=['Label']).copy()\n",
    "\n",
    "# 데이터 분할 (Feature & Label) - 변경된 컬럼명 사용\n",
    "X = filtered_data[SELECTED_FEATURES_RENAMED]\n",
    "y = filtered_data['Label']\n",
    "\n",
    "# 데이터 분할 (Train & Validation)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "print(f\"훈련/검증 데이터 분할 완료. 훈련 데이터 형태: {x_train.shape}\")\n",
    "\n",
    "# 모든 피처 컬럼을 강제로 숫자 타입으로 변환하고, 변환 불가 값은 결측치(NaN)로 처리\n",
    "for col in x_train.columns:\n",
    "    x_train[col] = pd.to_numeric(x_train[col], errors='coerce')\n",
    "    x_val[col] = pd.to_numeric(x_val[col], errors='coerce')\n",
    "    \n",
    "x_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "x_val.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리 (중앙값으로 대체)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "x_train = pd.DataFrame(imputer.fit_transform(x_train), columns=x_train.columns)\n",
    "x_val = pd.DataFrame(imputer.transform(x_val), columns=x_val.columns)\n",
    "\n",
    "# 스케일링\n",
    "scaler = RobustScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "\n",
    "# 인코딩\n",
    "le = LabelEncoder()\n",
    "y_train_final = le.fit_transform(y_train)\n",
    "y_val_final = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== XGBoost 모델 평가 결과 ==========\n",
      "정확도 (Accuracy): 0.9806\n",
      "정밀도 (Precision): 0.9567\n",
      "재현율 (Recall): 0.7378\n",
      "F1 스코어 (F1-Score): 0.7754\n",
      "============================================\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3603\n",
      "[LightGBM] [Info] Number of data points in the train set: 4328794, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.185555\n",
      "[LightGBM] [Info] Start training from score -3.589912\n",
      "[LightGBM] [Info] Start training from score -3.750624\n",
      "[LightGBM] [Info] Start training from score -2.134989\n",
      "[LightGBM] [Info] Start training from score -11.879602\n",
      "\n",
      "========== LightGBM 모델 평가 결과 ==========\n",
      "정확도 (Accuracy): 0.9686\n",
      "정밀도 (Precision): 0.7251\n",
      "재현율 (Recall): 0.6952\n",
      "F1 스코어 (F1-Score): 0.7050\n",
      "=============================================\n",
      "\n",
      "========== CatBoost 모델 평가 결과 ==========\n",
      "정확도 (Accuracy): 0.9804\n",
      "정밀도 (Precision): 0.9552\n",
      "재현율 (Recall): 0.7382\n",
      "F1 스코어 (F1-Score): 0.7750\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. XGBoost 모델 평가 ---\n",
    "xgb = XGBClassifier(\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb.fit(x_train_scaled, y_train_final)\n",
    "pred_xgb = xgb.predict(x_val_scaled)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_val_final, pred_xgb)\n",
    "precision_xgb = precision_score(y_val_final, pred_xgb, average='macro', zero_division=0)\n",
    "recall_xgb = recall_score(y_val_final, pred_xgb, average='macro', zero_division=0)\n",
    "f1_xgb = f1_score(y_val_final, pred_xgb, average='macro', zero_division=0)\n",
    "\n",
    "print(\"========== XGBoost 모델 평가 결과 ==========\")\n",
    "print(f\"정확도 (Accuracy): {accuracy_xgb:.4f}\")\n",
    "print(f\"정밀도 (Precision): {precision_xgb:.4f}\")\n",
    "print(f\"재현율 (Recall): {recall_xgb:.4f}\")\n",
    "print(f\"F1 스코어 (F1-Score): {f1_xgb:.4f}\")\n",
    "print(\"============================================\")\n",
    "\n",
    "\n",
    "# --- 2. LightGBM 모델 평가 ---\n",
    "lgbm = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "lgbm.fit(x_train_scaled, y_train_final)\n",
    "pred_lgbm = lgbm.predict(x_val_scaled)\n",
    "\n",
    "accuracy_lgbm = accuracy_score(y_val_final, pred_lgbm)\n",
    "precision_lgbm = precision_score(y_val_final, pred_lgbm, average='macro', zero_division=0)\n",
    "recall_lgbm = recall_score(y_val_final, pred_lgbm, average='macro', zero_division=0)\n",
    "f1_lgbm = f1_score(y_val_final, pred_lgbm, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n========== LightGBM 모델 평가 결과 ==========\")\n",
    "print(f\"정확도 (Accuracy): {accuracy_lgbm:.4f}\")\n",
    "print(f\"정밀도 (Precision): {precision_lgbm:.4f}\")\n",
    "print(f\"재현율 (Recall): {recall_lgbm:.4f}\")\n",
    "print(f\"F1 스코어 (F1-Score): {f1_lgbm:.4f}\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "\n",
    "# --- 3. CatBoost 모델 평가 ---\n",
    "cat = CatBoostClassifier(random_state=0, verbose=0)\n",
    "cat.fit(x_train_scaled, y_train_final)\n",
    "pred_cat = cat.predict(x_val_scaled)\n",
    "\n",
    "accuracy_cat = accuracy_score(y_val_final, pred_cat)\n",
    "precision_cat = precision_score(y_val_final, pred_cat, average='macro', zero_division=0)\n",
    "recall_cat = recall_score(y_val_final, pred_cat, average='macro', zero_division=0)\n",
    "f1_cat = f1_score(y_val_final, pred_cat, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n========== CatBoost 모델 평가 결과 ==========\")\n",
    "print(f\"정확도 (Accuracy): {accuracy_cat:.4f}\")\n",
    "print(f\"정밀도 (Precision): {precision_cat:.4f}\")\n",
    "print(f\"재현율 (Recall): {recall_cat:.4f}\")\n",
    "print(f\"F1 스코어 (F1-Score): {f1_cat:.4f}\")\n",
    "print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               2816      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,397\n",
      "Trainable params: 11,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "딥러닝 모델 훈련 시작...\n",
      "Epoch 1/100\n",
      "16910/16910 [==============================] - 48s 3ms/step - loss: 0.1708 - accuracy: 0.9654 - val_loss: 0.0892 - val_accuracy: 0.9770\n",
      "Epoch 2/100\n",
      "16910/16910 [==============================] - 47s 3ms/step - loss: 0.1201 - accuracy: 0.9758 - val_loss: 0.0802 - val_accuracy: 0.9779\n",
      "Epoch 3/100\n",
      "16910/16910 [==============================] - 47s 3ms/step - loss: 0.0897 - accuracy: 0.9767 - val_loss: 0.0789 - val_accuracy: 0.9781\n",
      "Epoch 4/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.1193 - accuracy: 0.9770 - val_loss: 0.0786 - val_accuracy: 0.9783\n",
      "Epoch 5/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.0848 - accuracy: 0.9773 - val_loss: 0.0980 - val_accuracy: 0.9782\n",
      "Epoch 6/100\n",
      "16910/16910 [==============================] - 45s 3ms/step - loss: 0.3267 - accuracy: 0.9775 - val_loss: 0.0774 - val_accuracy: 0.9784\n",
      "Epoch 7/100\n",
      "16910/16910 [==============================] - 45s 3ms/step - loss: 0.1562 - accuracy: 0.9775 - val_loss: 0.0776 - val_accuracy: 0.9787\n",
      "Epoch 8/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.0837 - accuracy: 0.9776 - val_loss: 0.4926 - val_accuracy: 0.9790\n",
      "Epoch 9/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.5802 - accuracy: 0.9777 - val_loss: 0.0762 - val_accuracy: 0.9787\n",
      "Epoch 10/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.3236 - accuracy: 0.9777 - val_loss: 0.0761 - val_accuracy: 0.9786\n",
      "Epoch 11/100\n",
      "16910/16910 [==============================] - 44s 3ms/step - loss: 0.0826 - accuracy: 0.9778 - val_loss: 0.0756 - val_accuracy: 0.9788\n",
      "Epoch 12/100\n",
      "16910/16910 [==============================] - 45s 3ms/step - loss: 0.0831 - accuracy: 0.9778 - val_loss: 0.0754 - val_accuracy: 0.9782\n",
      "Epoch 13/100\n",
      "16910/16910 [==============================] - 44s 3ms/step - loss: 0.2865 - accuracy: 0.9778 - val_loss: 0.0753 - val_accuracy: 0.9791\n",
      "Epoch 14/100\n",
      "16910/16910 [==============================] - 44s 3ms/step - loss: 0.0833 - accuracy: 0.9778 - val_loss: 0.0756 - val_accuracy: 0.9784\n",
      "Epoch 15/100\n",
      "16910/16910 [==============================] - 45s 3ms/step - loss: 0.2210 - accuracy: 0.9778 - val_loss: 0.0754 - val_accuracy: 0.9792\n",
      "Epoch 16/100\n",
      "16910/16910 [==============================] - 46s 3ms/step - loss: 0.0944 - accuracy: 0.9778 - val_loss: 0.0753 - val_accuracy: 0.9787\n",
      "Epoch 17/100\n",
      "16910/16910 [==============================] - 45s 3ms/step - loss: 0.0873 - accuracy: 0.9779 - val_loss: 0.0755 - val_accuracy: 0.9787\n",
      "Epoch 18/100\n",
      "16910/16910 [==============================] - 44s 3ms/step - loss: 0.1022 - accuracy: 0.9777 - val_loss: 0.0753 - val_accuracy: 0.9787\n",
      "모델 훈련 완료.\n",
      "33819/33819 [==============================] - 31s 912us/step\n",
      "\n",
      "학습 및 예측 수행 시간: 820.95초\n",
      "========== 딥러닝 모델 평가 결과 ==========\n",
      "정확도 (Accuracy): 0.9791\n",
      "정밀도 (Precision): 0.7542\n",
      "재현율 (Recall): 0.7032\n",
      "F1 스코어 (F1-Score): 0.7200\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # --- 1. 데이터 준비 (딥러닝 모델용 추가 전처리) ---\n",
    "# # 딥러닝 모델은 타겟(y) 데이터를 원-핫 인코딩(One-Hot Encoding)해야 합니다.\n",
    "# # 예: 3개 클래스 -> [0, 1, 2] 를 [[1,0,0], [0,1,0], [0,0,1]] 형태로 변환\n",
    "# num_classes = len(np.unique(y_train_final)) # 전체 클래스 개수\n",
    "# y_train_categorical = to_categorical(y_train_final, num_classes=num_classes)\n",
    "# y_val_categorical = to_categorical(y_val_final, num_classes=num_classes)\n",
    "\n",
    "# # 피처 개수 확인\n",
    "# num_features = x_train_scaled.shape[1]\n",
    "\n",
    "# # --- 2. 딥러닝 모델(MLP) 정의 ---\n",
    "# model = Sequential([\n",
    "#     # 입력층: input_shape에 피처 개수를 지정\n",
    "#     Dense(128, activation='relu', input_shape=(num_features,)),\n",
    "#     Dropout(0.3), # 과적합 방지를 위한 드롭아웃\n",
    "    \n",
    "#     # 은닉층\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "    \n",
    "#     # 출력층: 클래스 개수만큼 뉴런을 배치하고, 다중 분류를 위해 softmax 활성화 함수 사용\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # --- 3. 모델 컴파일 ---\n",
    "# # 학습 과정에 사용할 옵티마이저, 손실 함수, 평가 지표를 설정합니다.\n",
    "# model.compile(\n",
    "#     optimizer='adam', # 가장 일반적으로 사용되는 옵티마이저\n",
    "#     loss='categorical_crossentropy', # 다중 분류의 표준 손실 함수\n",
    "#     metrics=['accuracy'] # 학습 과정에서 정확도를 모니터링\n",
    "# )\n",
    "\n",
    "# # 모델 구조 요약 출력\n",
    "# model.summary()\n",
    "\n",
    "# # --- 4. 모델 훈련 ---\n",
    "# # 조기 종료(Early Stopping) 설정: 검증 손실(val_loss)이 5번 이상 개선되지 않으면 학습을 중단\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# print(\"\\n딥러닝 모델 훈련 시작...\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# history = model.fit(\n",
    "#     x_train_scaled, \n",
    "#     y_train_categorical,\n",
    "#     epochs=100, # 최대 100번 학습 반복\n",
    "#     batch_size=256, # 한 번에 256개씩 데이터를 묶어 학습\n",
    "#     validation_data=(x_val_scaled, y_val_categorical),\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1 # 학습 과정 출력\n",
    "# )\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"모델 훈련 완료.\")\n",
    "\n",
    "# # --- 5. 최종 예측 및 평가 ---\n",
    "# # 딥러닝 모델의 predict 결과는 각 클래스에 대한 확률값이므로, 가장 높은 확률의 인덱스를 찾아야 함\n",
    "# pred_probs = model.predict(x_val_scaled)\n",
    "# pred_labels = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# # 최종 평가 지표 계산 (평가 시에는 원본 y_val_final 사용)\n",
    "# accuracy = accuracy_score(y_val_final, pred_labels)\n",
    "# precision = precision_score(y_val_final, pred_labels, average='macro', zero_division=0)\n",
    "# recall = recall_score(y_val_final, pred_labels, average='macro', zero_division=0)\n",
    "# f1 = f1_score(y_val_final, pred_labels, average='macro', zero_division=0)\n",
    "\n",
    "# # 최종 결과 출력\n",
    "# print(f\"\\n학습 및 예측 수행 시간: {end_time - start_time:.2f}초\")\n",
    "# print(\"========== 딥러닝 모델 평가 결과 ==========\")\n",
    "# print(f\"정확도 (Accuracy): {accuracy:.4f}\")\n",
    "# print(f\"정밀도 (Precision): {precision:.4f}\")\n",
    "# print(f\"재현율 (Recall): {recall:.4f}\")\n",
    "# print(f\"F1 스코어 (F1-Score): {f1:.4f}\")\n",
    "# print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 훈련된 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 최적 모델이 '../../model/traffic/traffic_model.joblib'에 저장되었습니다.\n",
      "결측치 대체기가 '../../model/traffic\\imputer.joblib'에 저장되었습니다.\n",
      "스케일러가 '../../model/traffic\\scaler.joblib'에 저장되었습니다.\n",
      "레이블 인코더가 '../../model/traffic\\label_encoder.joblib'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델을 저장할 폴더가 없으면 생성\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "# 1. 모델 저장\n",
    "joblib.dump(xgb, MODEL_PATH)\n",
    "print(f\"학습된 최적 모델이 '{MODEL_PATH}'에 저장되었습니다.\")\n",
    "\n",
    "# 2. 결측치 대체기(Imputer) 저장  <- 이 부분 추가!\n",
    "IMPUTER_PATH = os.path.join(os.path.dirname(MODEL_PATH), \"imputer.joblib\")\n",
    "joblib.dump(imputer, IMPUTER_PATH)\n",
    "print(f\"결측치 대체기가 '{IMPUTER_PATH}'에 저장되었습니다.\")\n",
    "\n",
    "# 3. 스케일러(Scaler) 저장\n",
    "SCALER_PATH = os.path.join(os.path.dirname(MODEL_PATH), \"scaler.joblib\")\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(f\"스케일러가 '{SCALER_PATH}'에 저장되었습니다.\")\n",
    "\n",
    "# 4. 레이블 인코더(LabelEncoder) 저장\n",
    "ENCODER_PATH = os.path.join(os.path.dirname(MODEL_PATH), \"label_encoder.joblib\")\n",
    "joblib.dump(le, ENCODER_PATH)\n",
    "print(f\"레이블 인코더가 '{ENCODER_PATH}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A_P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
